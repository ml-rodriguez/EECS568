\documentclass[letterpaper, 10pt, conference]{ieeeconf}

\IEEEoverridecommandlockouts    % If we want to use the thanks command

\usepackage{fullpage}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{subfig}
\usepackage{algorithmic}
\usepackage{algorithm}

% EPS2PDF stuff?

\newcommand{\xxx}[1]{\textcolor{red}{#1}}

\title{\LARGE \bf
    3D Environment Reconstruction Using RGB-D Visual Odometry
}

\author{Pedro D'Aquino, Rob Goeddel, Lauren Hinkle, and John Peterson}


\begin{document}

\maketitle
%\thispagestyle{empty} % XXX These were in the IROS example, but I didn't read
                      % far enough along to see if they disappear later
%\pagestyle{empty}

\begin{abstract}
The reconstruction of environments in 3D can be challenging with just range
data. RGB-D sensors like the Microsoft Kinect provide visual details that
allow odometry to be used even in environments with ambiguous depth data.
This paper demonstrates a simplified implementation of Henry et al.'s work
on RGB-D Mapping.~\cite{Henry2010rgbd}
Our system can construct reasonably accurate models of
small to medium sized indoor environments in real time at resolutions of up
to 2\,cm and in near-real time at 1\,cm. By using SURF features from the image
data in conjunction with the Kinect's depth data, we are able to compute
accurate alignments of point clouds over time. Our system does not attempt to
implement the loop-closing portion of Henry et al.'s work.
\end{abstract}

\section{Introduction and Motivation}
This paper will present a system capable of building detailed colored point
clouds using a consumer grade RGB-D sensor, the Microsoft Kinect. The
convenient availability of such sensors makes them an appealing budget option
for a small robot operating in an indoor environment where the limited range
of the Kinect is less of a concern. Furthermore, the colored point data
generated by RGB-D sensors can provide more robust alignments than even high
quality pure-range sensors such as LIDAR in environments with range-detectable
features.

LIDAR and other range based sensors may be used to generate extremely detailed
and accurate 3D point clouds that may be merged over time to reconstruct a 3D
model of an environment. Generally, these alignments are straightforward to
detect and such systems are fairly robust. However, there are some weaknesses.
A classic example is the so-called ``hallway problem.'' A common concern for
SLAM systems is that all stretches of a long hallway look the same. The lack
of distinguishing features mean that, while we may correctly align the walls
to run parallel to previously observed walls, it is difficult if not
impossible to constrain an observation to one particular location. Depending
on the reliability of a robot's odometry, this may cause compression or
expansion of the hallway in question. Features from associated image data can
make an otherwise indistinguishable segment of wall easy to align with
previous scans, as seen in Fig.~\ref{fig:cse-wall}.

The output of an RGB-D sensor can also be used for general 3D modeling
applications. Artists may spend days creating detailed models of objects from
the real world. Given an RGB-D sensor and a system capable of merging many
observations, an untrained user can wave the sensor around the object needing
to be modeled and automatically generate a 3D model complete with texture.
This preservation of visual data even aids in the end product of the mapping
task. Imagine a 3D map of a building created from range data. While the map
might be an accurate layout of the building, the lack of visual cues may still
leave the user feeling lost and confused while navigating through a virtual
reconstruction of this environment. Associated color data could
have features such as door numbers and maps of the building available to aid
in navigation.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/Ed_hugging_pole.png}
\caption{A model of Prof. Ed Olson taken during the EECS 568 public showcase.
Voxel resolution for this model is 0.5\,cm, allowing us to preserve details
such as the creases in Ed's shirt and pants or the pupils of his eyes.}
\label{fig:ed-model}
\end{figure}

% Figure spanning columns. Top of second page, probably
\begin{figure*}[t]
\centering
\includegraphics[width=.9\textwidth]{figures/CSE_Wall.png}
\caption{By using image features such as the corners of rocks in a wall,
combined color and depth data can be accurately placed into a global map even
in flat hallway situations. As seen above, even when depth features such as
the doorways and recycling bins go out of view, our system continues to build
an accurate model of the wall to the left of the doors.}
\label{fig:cse-wall}
\end{figure*}

\section{Background}
Our work is a simplified reimplementation of the work of Peter Henry et
al~\cite{Henry2010rgbd}. Their work demonstrated a system capable of RGB-D
mapping using a conjunction of 3D SIFT alignment, plane-fitting ICP, and the
SLAM system TORO. By using visual features projected into 3D space to gain an
initial alignment between frames and by using ICP to converge the rest of the
way, Henry et al. were able to make a ``near-real time'' system capable of
producing very detailed 3D reconstructions. By maintaining periodic keyframes
for different locations, they are able to make large-scale loop closures by
recognizing those same features later, resulting in a very robust system.

To show their results, Henry et al. created what they call \emph{surfels}.
Surfels allow them to deal with the massive amounts of data provided by their
RGB-D sensor without losing significant detail about the scene. As videos of
their results demonstrate, the system is capable of preserving details such as
the pattern on carpet or slightly blurry versions of people's faces in
pictures.

Albert Huang and his group at MIT, working in conjunction with Henry et al. at
the University of Washington, were able to take this system a step further,
creating a real-time mapping system that allowed a quadrotor to autonomously
fly through cluttered 3D environments using a Kinect~\cite{Huang2011isrr}.
Huang et al.'s work adds data from IMUs and switches to FAST features to
support real time voxel mapping at a coarse resolution sufficient for
trajectory planning.

\section{Methodology}
Our implementation of RGB-D mapping passes entirely on the SLAM portion,
focusing instead on feature-based odometry. Lacking time to implement a
full RGB-D SLAM system capable of rendering real-time navigable scenes at the
level of detail demonstrated by Henry et al., we focused on making our system
real-time. This can be used to justify many of deviations from the original
implementation.

\subsection{Kinect Data Acquisition and Calibration}
We use the open source OpenKinect library (often referred to as libfreenect)
to acquire data from the Kinect~\cite{OpenKinect}. Though our project is
written primarily in Java, our interface to the Kinect is written in C and we
use JNI bindings to tie the C and java together. We are easily able to acquire
a full 30 frames per second with this code.

We calibrated our Kinect using Nicolas Burrus's RGBDemo
toolkit~\cite{BurrusCalibration}. The software includes a tool for calibrating
a Kinect simply by taking a large number of pictures of a calibration target
with both the IR and RGB cameras. This tool not only returns the intrinsics
and undistortion parameters for both cameras, but also the stereo calibration
parameters relating the two cameras to each other. Using these parameters
along with a depth model provided by Dr. St\'{e}phane Magnenat on the
OpenKinect website, we are able to construct a colored point cloud with only
minimal blurring of color between objects.

\subsection{Frame Matching Using Image Features}
The first approximation of the movement of the Kinect comes from matching 3D image
features between the current and the last frame. In this section, we describe the details
of our implementation.

\subsubsection{Feature Extraction and Description}
We compute SURF features and descriptors
to extract and describe features in the image~\cite{Bay06surf}. Our approach differs
from Henry et al., who use a parallel implementation of SIFT descriptors~\cite{SIFTGPU}.
As the majority of the development laptops we used had integrated graphics cards, using
a GPU implementation was not feasible. We found that SURF was an adequate tradeoff
between robustness and performance. Our implementation uses OpenCV~\cite{opencv_library}
for feature detection and descriptor extraction, with a JNI wrapper to allow for
communication with the rest of the system.

\subsubsection{Feature Matching}
We match a SURF feature to its nearest Euclidian neighbor in the other frame. The matching
uses a KD-tree for efficiency. We experimented with reducing the number of outliers by
constraining the matches in two ways. In the ``distant second'' approach, let $A_i$ be the
best match for feature $B_j$, with distance $d_{ij}$, and $A_k$ be the second best match,
with distance $d_{kj}$. Then we only consider $A_i$ to match $B_j$ if $d_{ij} \le \alpha d_{jk}$.
We also experimented with enforcing ``marriage'' between matches: $A_i$ matches $B_j$ only if
$B_j$ also matches $A_i$.

Both constraints reduced the number of wrong matches, but also eliminated some inliers.
Additionaly, there was significant overhead from applying the ``marriage'' constraint.
In the end, we found that applying RANSAC as explained in the next section was enough
to robustly filter outliers.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/awesome_couch.png}
\caption{A reconstruction of the couches in Tishman Hall.}
\label{fig:couch}
\end{figure*}

\subsubsection{Computing the Rigid Body Transformation}
\label{ransac}
After features are extracted using SURF from the RGB images, we project
them into 3D space using the depth data from the Kinect. Given these
corrseponding 3D features, we may compute good alignment using Horn's
algorithm and RANSAC.

Horn's algorithm is a closed-form calculataion of the rigid body
transformation in three or more dimensions.  Given a set of corresponding
points it calculates the optimal transformation without iteration
~\cite{horn1987closed}.  The majority of the calculations we perform with
Horn's algorithm have only three sets of corresponding 3D points, and so
Horn's algorithm is quite fast.

Horn's Algorithm is used at every iteration within our RANSAC implementation.
We use RANSAC to discover outliers in the set of corresponding features
set.  At each iteration of RANSAC, three corresponding pairs of features
are selected randomly; this is the minimum needed to model a rigid body
transformation.  Horn's method is used on the projected locations of these
features, and the resulting transformation is voted upon by all the remaining
features.  Features from the first frame that are transformed to within $1cm$
of their corresponding feature in the second frame and considered a vote in
favor of the transformation and are considered inliers.  The largest set of
inliers is maintained from iteration to iteration.

RANSAC halts after a given number of iterations or after $80\%$ of the
correspondences are considered to be inliers.  This was done to decrease
the time spent in RANSAC, however in practice, inlier sets rarely exceeded
$80\%$ of the total number of corresponding features.  An initial
transformation is then computed using just the optimal inlier set.

\subsection{ICP}
	The rigid body transformation (RBT) calculated by aligning corresponding visual features can be further refined by aligning point clouds between successive frames.  Iterative Closest Point (ICP) is one method to accomplish this.  ICP is an iterative processes that alternates between using a given rigid body transformation to determine point correspondences and using these point correspondences to compute an RBT to align the points.  \newline

% XXX Oh man! Such effort, here. Looks good, but for future, pain-saving
% reference, my friend, let me tell you about a wonderful tool called
% \begin{algorithmic}! Your life will be much happier with the
% algorithmic block to save the day.

\begin{algorithm}
\begin{algorithmic}
\REQUIRE pointclouds $A = a_{i}$, $B = b_{i} $, initial transformation $T_{0}$
\STATE $T = T_{0}$
\WHILE {\emph{not converged}}
\FOR {$i = 1 \to N$}
\STATE $a^{B}_{i} \in B = T * a_i$
\STATE $n_{i} =$ nearested point in $B(a^{B}_{i})$
\ENDFOR
\STATE $T = argmin_{T} \{\sum_{i} ||T * a_{i} - n_{i} ||^2\}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

%$\textbf{given: } \text{pointclouds } A = a_{i}, B = b_{i} $ \newline
%$\text{An initial transformation, $T_{0}$} $ \newline
%$T = T_{0}; $\newline
%$\textbf{while} \emph{ not converged} \textbf{ do} $\newline
%$\hspace*{5 mm} \textbf{for } \emph{i = i:N} $\newline
%$ \hspace*{10mm} a^{B}_{i} in B = T * a_i$ \newline
%$ \hspace*{10 mm} n_{i} = Find Nearest Point in B(a^{B}_{i}) $ \newline
%$ \hspace*{5 mm} \textbf{end} $ \newline
%$ \hspace*{5 mm} T = argmin_{T} \{\sum_{i} ||T*a_{i} - n_{i} || ^2\}; $ \newline
%$ \textbf{end} $ \newline

	Let point cloud A be the point cloud corresponding to the most recent frame received from the Kinect and let point cloud B be the previous one received.  ICP applies an initial estimate of the RBT between $A$ and $B$, to the each of the points, $a_{i}$ in $A$ to get points a in frame $B$.  Simple nearest neighbor data association determines point correspondences which is accelerated through the use of a K-D tree.  Then using Horn's algorithm we compute the optimal RBT to align the corresponding points~\cite{horn1987closed}.  We made use of the AlignPoints3D April code for this computation.  Then we compute the error in the alignment by taking the sum of the distances between corresponding points.  This whole process is repeated until the decrease in error from one iteration to the next has increased above a set threshold.

	Unfortunately, simple ICP as described above has quite a few weaknesses, some of which we have attempted to address.

	ICP uses a point to point error metric which implicitly assumes that our sensor has measured the same point from both frames.  However this is not a completely rigorous or robust way of describing how two views of a scene are related to one another, since it is exceedingly unlikely that our sensor will sample the exact point in each view.  Alternative formulations of ICP, such as Iterative Closest Plane, which minimizes a point to plane error metric deal with this problem to some degree.  Iterative Closest Plane introduces the additional complication of computing normal vectors for each point in one of the views.  This more complicated formulation would yield improved accuracy, however there are other areas where we could improve the project that would provide greater benefits for less cost in development time, so we avoided utilizing this more difficult formulation.

	ICP's nearest neighbor data association also implicitly assumes that the two point clouds being matched completely overlap.  In the vast majority of cases, movement of the sensor means that there will be points in one point cloud that do not correspond to points in the other point cloud.  Inaccurate point correspondences lead to distortions in the RBT.  The absence of corresponding point in the other view can occur for points that are near occlusion boundaries or close to the maximum range of the sensor.  It would be possible to construct a heuristic that would reason about occlusions and sensor range to avoid matching such points.  As a substitute for this more complex reasoning we introduce a horizon for point correspondences which limits the maximum distance between corresponding points.   If the nearest neighbor if beyond this horizon, then the correspondence is discarded.  However, this horizon introduces other complications.  If it is set to be too short, then the initial alignment will need to be very good to make any correspondences at all.  On the other hand if it is too long, then there will be no effect on the correspondences that are made.  In practice the alignment that RANSAC provides is good enough  to set a short horizon which all but eliminates this problem.

	There is still one, more troubling issue that a data association horizon introduces.  If the algorithm uses an error metric that is simply the sum of the distances between corresponding points, then it is plausible for the error to increase as the alignment improves if this horizon is used.  Given two point clouds and an initial alignment that puts most of the points beyond the horizon for data association.  The initial error will be relatively small because only a small number of correspondences will exist between the two clouds.  As the alignment is improved, more point correspondences will be discovered, which would cause the total error to increase.  This could cause the algorithm to terminate prematurely, leading to suboptimal alignments.  To solve this issue we use a normalized error metric which is simply the average error per correspondence.  Using this metric, the error will decrease correctly as the alignment is improved.

	ICP also requires a good initial alignment in order to guarantee that it converges to the global minimum error.  If we have a poor initialization, ICP will get caught in a local minimum.  Using visual feature correspondences provides a good enough estimate of the RBT that this problem is completely taken care of.

	Another major issue with ICP and other scan matching schemes like Mult-Resolution scan matching is that some scene geometries can make motions ambiguous.  For example moving down the length of a hallway would yield no change in the 3D range scans read by the Kinect or other similar sensors making it impossible to tell that the sensor has moved at all.  In fact, ICP would converge to a solution that would claim that the sensor has not moved at all, since such a solution would yield the greatest overlap of the two point clouds.  If such an environment has visual features, we can use these features to augment the alignment algorithm, as originally described in Henry's paper~\cite{Henry2010rgbd}.  This modification, described below, yields significantly improvements even in cases where there were no ambiguities in the environment.

\subsection{RGBD-ICP}
Using visual features to augment point cloud matching improves the quality of alignments produced by this step.    To utilize visual features we introduce the 3D coordinates relative to the Kinect, of inlier feature correspondences, into both point clouds.  Since their correspondence has already been determined by matching SURF descriptors and then filtered with RANSAC, these correspondences remain constant throughout each iteration of ICP.

\begin{algorithm}
\begin{algorithmic}
\REQUIRE pointclouds $A = a_{i}, B = b_{i}$, initial transformation $T_{0}$, corresponding visual features $Fa_{i}, Fb_{i}$
\STATE $T = T_{0}$
\WHILE {\emph{not converged}}
\FOR {$i = 1 \to N$}
\STATE $a^{B}_{i} \in B = T * a_i$
\STATE $n_{i} = $ nearest point in $B(a^{B}_{i})$
\IF {$||n_{i}-a^{B}_{i}|| \leq d_{threshold}$}
    \STATE $w_i = 1$
\ELSE
    \STATE $w_i = 0$
\ENDIF
\ENDFOR
\STATE $T = argmin_{T} \{\sum_{i} w_{i}||T*a{i} - n_{i} ||^2 + m_{i} ||T*Fa_{i} - Fb_{i}||^2\} $
\ENDWHILE
\end{algorithmic}
\end{algorithm}

%$\textbf{given: } \text{pointclouds } A = a_{i}, B = b_{i} $ \newline
%$\text{An initial transformation, $T_{0}$} $ \newline
%$\text{Corresponding Visual Features } Fa_{i}, Fb_{i} $ \newline
%$T = T_{0}; $\newline
%$\textbf{while} \emph{ not converged} \textbf{ do} $\newline
%$\hspace*{5 mm} \textbf{for } \emph{i = i:N} $\newline
%$ \hspace*{10mm} a^{B}_{i} in B = T * a_i$ \newline
%$ \hspace*{10 mm} n_{i} = Find Nearest Point in B(a^{B}_{i}) $ \newline
%$ \hspace*{10 mm} \textbf{if } ||n_{i}-a^{B}_{i}|| <= d_{threshold} $ \newline
%$\hspace*{15 mm} w_i = 1; $ \newline
%$\hspace*{10mm} \textbf{else} $ \newline
%$\hspace*{15mm} w_i = 0; $ \newline
%$\hspace*{10mm} \textbf{end} $\newline
%$ \hspace*{5 mm} \textbf{end} $ \newline
%$ \hspace*{5 mm} T = argmin_{T} \{\sum_{i} w_{i}||T*a_{i} - n_{i} || ^2 +$ \newline
%$ \hspace*{15mm} m_{i} ||T*Fa_{i} - Fb_{i}||^2\}; $ \newline
%$ \textbf{end} $ \newline


	Visual features are generally out-numbered by other points in the point cloud, so we have introduced a correspondence weighting term and modified the AlignPoints3D class to utilize this weight when computing the optimal transformation.  By increasing the weight of visual features relative to other points in the point cloud we can ensure that they influence the alignment to the degree that we want.  To achieve a relative weight of 50\% where as a whole both the point cloud and the visual features have approximately same say in the final alignment, each visual feature correspondence is given a weight, $m_{i}$, equal to the size of the point cloud divided by the number of feature correspondences.  It would be more accurate to calculate the number of point correspondences and use that rather than the size of the point cloud.  Conceptually this weighting procedure is equivalent to duplicating each of the visual features to increase their influence on the alignment.  However this formulation for the weight becomes problematic at low inlier correspondence numbers as increasingly few features are given greater and greater weight when in fact we should trust visual features less if there are so few to use in the first place.  To address this issue, the maximum weight assigned to each visual feature is capped at a maximum number ensuring that the influence of the point cloud grows as visual features become less reliable.

\subsection{Constant Velocity Motion Model}
In circumstances when there are few or no visual features, we would still like to estimate the RBT between frames to initialize ICP.  As in the paper, we implemented a constant velocity model which uses the RBT estimated by RGBD-ICP at each time step combined with a timer to estimate the linear and angular velocities of the Kinect.  These values averaged over time to attempt to reduce the noise that velocity estimates are prone to, however this tends to introduce time delays in the response of the estimated velocity compared to the actual velocity of the Kinect.  We found that once the set of inliers features from RANSAC dropped to about 20, it would no longer give accurate RBTs.  At this threshold, the constant velocity motion model steps in and provides an estimate to initialize ICP.

By definition the constant velocity model assumes that the Kinect maintains a constant velocity so large changes in the motion of the sensor in areas without visual features causes the system to fail.  Additionally there are cases where both a lack of visual features and the presence of a geometric ambiguiity cause both RANSAC and RGBD-ICP to fail.  This simultaneous failure quickly renders the constant velocity model useless since its estimates are quickly ruined by the erroneous results from RGBD-ICP.  We attempted to deal with this problem by setting internal limits on how rapidly the estimates of velocity could change, but our implementation currently has no heuristics to determine when RGBD-ICP has failed leading to poor behavior in circumstances when the constant velocity model alone might yield better results.  Improvements in this area would greatly increase the reliability of the system in many common situations such as mapping large empty sections of large rooms, where features are beyond the range of the sensor or if the sensor is looking at a featureless wall for brief periods of time.



\subsection{Rendering the Scene}
We do not attempt to implement the surfels demonstrated by Henry et al.
Surfels, while excellent at preserving detail, are a huge engineering
undertaking in and of themselves, and it is extremely costly to
update their values during data collection. Surfels alone would destroy our
chances of rendering updates to the scene in real time. Instead, we chose to
store data as voxels of a fixed resolution. Voxel updates, while not free, are
incredibly cheap in comparison to surfel updates and
individual voxels do not change in memory requirements over time. Our
voxels store a running total of the ARGB values observed in that bin and are
treated as being colored as an average of these observations.

Henry et al. wrote a custom tool to allow them to navigate surfel environments
in real time~\cite{Henry2010rgbd}. Again, this was a large engineering effort
and was not our main priority. Instead, we render all of our voxels at once,
as cubes, regardless of on screen visibility. The performance for this
rendering scheme proved to be unacceptable, so our voxels are instead rendered
as individual points rendered at the center of their respective bins. As a
result, scenes are best viewed from some distance so as to mask the gaps
between the points. To facilitate 3D navigation through the scene, we employ a
Logitech gamepad supporting translation in the current camera plane as well as
in place rotation of the camera.

\section{Evaluation}
\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/cropped_gt_slc.png}
\caption{The reconstruction of one of the whiteboards in the student learning center,
in the CSE building. This dataset was used to evaluate the system.}
\label{fig:gt-slc}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/gt_translational.pdf}
\caption{The errors in the x, y and z coordinates as the Kinect moves between the checkpoints
for the two datasets.}
\label{fig:plot-trans}
\end{figure}

\subsection{Ground Truth Evaluation}
The end goal of our system is to provide rich 3D reconstructions of indoor environments.
It is hard to quantify the quality of those reconstructions, and therefore to numerically
judge the performance of the system. However, we can estimate the error in the visual odometry,
which is the most important factor in the reconstruction of the environment. This provides
a decent approximation of the quality of the final output.

Our first try at an experiment involved using AprilTags~\cite{olson2011tags} to
provide ground-truth localization. Because the size of the tags are known, it is
possible to know their 3D positions relative to the camera. If we assume that the tags are fixed and that
the detection of the tags is precise enough, we can use the movement of the
Kinect in relation to the tags as the ground-truth. However, we have found that
the RGB images were not of sufficient quality to provide accurate and consistent
detection of the tags. In particular, debayering artifacts were clearly noticable.
This is problematic because the tags are black and white. The debayering blurs the
image around the edges in such a way that the detected position of the tags is very noisy.

We therefore resorted to a simple ground-truth experiment. Instead of measuring the actual trajectory of
the Kinect, we physically marked a number of checkpoints on the world,
and moved the Kinect from one to the other. At every one of those checkpoints, we recorded the
Kinect's estimated position, with the first checkpoint being at the origin. At the end of the
test run, we compared the Kinect's esimate of the checkpoints to where they really were in the
world.

We performed this evaluation in two environments. Fig.~\ref{fig:cse-wall} shows the reconstruction of a
feature-rich hallway. Three checkpoints were placed on the ground along the hallway, and the Kinect was
moved between them. Fig.~\ref{fig:gt-slc} is the reconstructed whiteboard in the student learning
center in CSE. For this test, six checkpoints were placed in a zig-zag pattern on a table.

Fig.~\ref{fig:plot-trans} shows the translational error at every checkpoint for both datasets.
As expected from a pure odometry system, the error acumulates and the estimative of the Kinect's
position tends to diverge with time. Fig.~\ref{fig:plot-angular} presents the angular error at
every checkpoint. The ``table" dataset shows significantly greater errors because of the
zig-zag pattern of the markers, which caused angular errors to accumulate more rapidly.

We also conducted a number of reconstructions without ground-truth data. Fig.~\ref{fig:couch}
is an example of a reasonably long trajectory for which our system produced a good
reconstruction.

\subsection{Real-time Performance}
Given our simplification of the original problem (namely not implementing SLAM
or surfels), one of our major goals was to be able to collect and add data to
our global point cloud in real-time. By switching from SIFT to SURF, for
example, we reduced the cost of processing a frame for features from
~180\,ms per frame to ~9\,ms per frame. The total cost of processing a frame
for a variety of voxel resolutions
can be seen Table~\ref{tbl:match-times}. Generating voxels clearly dominates the
processing time, taking 78\% of the total time for 1\,cm of resolution
(generally the smallest resolution we operate at). Still, this is as opposed
to surfel updates in Henry et al.'s work, which were reported to take roughly
6 seconds per frame, so we still consider this a vast improvement. Though by
no means fast enough to keep up with the full speed of the kinect, this allows
us to update scenes at between 7-20\,fps depending on our needs.

\begin{table}
    \centering
    \begin{tabular}{ | c || r | r | r | }
    \hline
    Resolution & 1\,cm  & 2\,cm & 4\,cm \\
    \hline
    \hline
    SURF & 9\,ms &9\,ms & 9\,ms \\
    \hline
    RANSAC & 16\,ms & 16\,ms & 16\,ms\\
    \hline
    ICP & 5\,ms & 5\,ms & 5\,ms \\
    \hline
    Voxelization & 105\,ms & 47\,ms & 21\,ms \\
    \hline
    {\bf Total} & {\bf 135\,ms} & {\bf 77\,ms} & {\bf 51\,ms} \\
    \hline
    \end{tabular}
\caption{Time spent at each stage of point cloud processing for varying voxel
    resolutions. As expected, voxels are still extremely costly to process,
    but small reductions in voxel resolution lead to dramatically improved
    results.}
\label{tbl:match-times}
\end{table}


Real time rendering performance is another issue. Though we are able to
process the data quickly enough and add it to our global point cloud in
real-time, rendering the frame is incredibly costly, especially as the global
point cloud grows. A 30 second dataset collected of a medium sized office
contains over 2 million voxel points at 1\,cm resolution, and visual
updates/real-time navigation of this scene lag noticeably behind input. Our
experience shows that the system delivers real-time visual updates and
navigability when running with 2\,cm voxels. For live demonstrations, this is
the resolution we employ. Rendering times obviously increase with the number
of points in the scene, as we do nothing to stop rendering points we cannot
see. However, even after only several seconds of data collection, rendering
the scene at 1\,cm resolution takes over half a second whereas a similar scene
at 2\,cm or 4\,cm resolution takes between 200-250\,ms. This means that,
despite our ability to process data faster than this, updates to the scene
appear slowly from the viewer's perspective. We believe that 4-5\,fps falls
clearly into the realm of real-time, though, and found this performance
adequate for demonstration purposes.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{figures/KinectDemo.png}
\caption{RGB and depth data from a Kinect. For easier consumption, the depth
data has been mapped through the colors of the rainbow where red corresponds
to objects roughly 1\,m away and purple corresponds to objects roughly 8~\,m
away. Though the maximum range is at least 8\,m, the depth data
becomes increasingly noisy, so we only use data within 3.8\,m of the Kinect,
the range at which the data seems to begin to deteriorate.}
\label{fig:kinect-demo}
\end{figure}

\subsection{Public Demonstration}
To show how our system works, we employ two separate demonstrations. The first
is a simple Kinect testing demo that we used to verify that we were receiving
good color and depth data (seen in Fig.~\ref{fig:kinect-demo}. This demo helps
us explain the type of data that a Kinect gives us and how we can use that to
construct 3D point clouds in the first place. Our second demonstration
consists of running our system live at a slightly lowered resolution so that
point data is clearly added to our model of the world as we move around. By
allowing people to fly through our virtual world as we are actively building
it, they can easily compare the virtual scene to the real one before them. We
can also build detailed 3D models of people on the fly, if the are able to
hold still for a long enough period of time, as seen in Fig.~\ref{fig:ed-model}.

\section{Conclusion}
We successfully implemented a simplified version of Henry et al.'s RGB-D mapping
system that is able to make 3D reconstructions of the surrounding environment
using a Kinect sensor~\cite{Henry2010rgbd}. Modifications to the original
design allow our system to run in real time, though we are unable to keep up with
the full speed of our sensor, forcing us to throw away some data. The addition of
visual features to depth information allow us to accurately maintain our position
estimate in the world even in environments lacking unique depth data. Though our
system could clearly benefit from the addition of a full SLAM system to
correct our slowly accumulating error, as exemplified in Fig.~\ref{fig:slam},
 we are still able to maintain position
estimates within a 10\% error margin of the total distance traveled.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/gt_angular.pdf}
\caption{The errors in roll, pitch and yaw as the Kinect moves between the checkpoints
for the two datasets.}
\label{fig:plot-angular}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/slam.png}
\caption{An example of a dataset that would have greatly benefitted from loop-closures.}
\label{fig:slam}
\end{figure}

\section*{Acknowledgements}
Special thanks to Pradeep Ranganathan for providing some extra depth
information in Fig.~\ref{fig:kinect-demo} and Prof. Ed Olson for patiently
posing for us during our final demonstration.

% XXX References
\bibliographystyle{IEEEtranS}
\bibliography{sources}

\end{document}
