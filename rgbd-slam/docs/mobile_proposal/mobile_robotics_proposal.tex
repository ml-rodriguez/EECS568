\documentclass[12pt]{article}

\begin{document}

\title{3D Environment Reconstruction Using An RGB-D Sensor}
\maketitle

{\bf Team Members Uniquenames}
jrpeters, pdaquino, lhinkle, rgoeddel
\\ \\
{\bf Project Summary }\\
We will develop a system that constructs a 3D map of an environment using data extracted
from a Kinect that has explored the region.  This will be accomplished by extracting features
from the Kinect's color data in each frame and using RANSAC on these features to align
frames.
\\ \\
{\bf Use of Existing Tools }\\
We will use some of the april library (vis, jmat, etc) and possibly lcm for saving and
transferring data.  In addition, we will be using basic code from libfreenect to get data from
 the Kinect, and OpenCV (or some variant) to gather SIFT features.
\\ \\
{\bf Pedagogical Merit }\\
This project will teach us about performing visual odometry.  We will develop matching
methods for RGBD data, beginning with three degrees of freedom and hopefully increasing
to six degrees of freedom.  We will do this with a RANSAC based algorithm, and may use ICP
to improve our results.

We will explore methods for visualizing the increasingly large 3D meshes in reasonable time
as our map gets larger. Our current plan is to sparsely store voxels and to perform na\"{i}ve
color averaging on points that fall into the various voxels.
\\ \\
{\bf Development Trajectory }\\
We will begin by constraining the Kinect to only three degrees of freedom in its movement: x
and y translation and yaw.  To initially simplify the problem we will add artificial features, such
as April Tags, to the environment to enhance feature detection and tracking.  The resulting
maps will be displayed with voxels.  As we develop and improve our algorithms, we will remove
the artificial features.

If time allows, we will increase the degrees of freedom for the movement of the Kinect and include
loop closures to improve our maps (in that order).
\\ \\
{\bf Demonstration }\\
We will build a 3D map of an environment.  If our system runs in close to real time (as is our goal)
we will do this as a demonstration, allowing people who come to our booth to take the Kinect on a
walk around the room and see the map they made.  If our system does not run in real time, we will
have a video of the system performing alongside a virtual world generated by that run that the people
passing by can do a fly-through of.
\\ \\
{\bf Management Plan }\\
Our group will meet on Tuesdays, Thursdays, and periodically on weekends.  John and Pedro,
who are both in Computer Vision this semester, will be focusing on obtaining data and features
from the Kinect.  Pedro will be doing feature extraction with the help of OpenCV, and porting
results from C++ to Java.  John will work on calibrating the Kinect, and projecting the data into
3D.  Lauren will work on aligning features between frames using RANSAC.  Rob will work on efficiently
organizing the data and displaying it.
\\ \\
{\bf Checkpoint Result }\\
On December 8th we will have a short video of a virtual room generated using data from the Kinect
displayed next to a video of what the Kinect ``saw'' as it was rotated in the room and collected data.
\\ \\
{\bf Safety }\\
There are no anticipated safety issues beyond normal risks associated with using a computer.
\\ \\
{\bf Resources }\\
We will be using a Kinect, which has already been acquired by our team.
\end{document}
