\documentclass[12pt]{article}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath, algorithmic, color, multicol}
\usepackage{subfigure}


\title{Progress Report}
\author{
	John Peterson (jrpeters)\\
	Paridhi Desai (paridhid)\\
	Pedro d'Aquino (pdaquino)}

\begin{document}
\maketitle

\section{Introduction}
For mobile robots to be useful in the real world they must be able to move through the world 
with the same ease and proficiency that the humans they are assisting do.  To navigate the world, 
mobile robots must first localize and map their surroundings, in s step called SLAM, before they are 
able to apply other path finding algorithms to choose their actions.   Unfortunately SLAM usually relies 
on expensive and numerous range sensors which significantly raise the cost of any mobile robot. Cameras
offer an interesting solution to this problem.  They are significantly cheaper and more versatile than the 
sensors customarily used in SLAM applications, such as laser scanners.

For this project, we will investigate solving the SLAM problem using a Microsoft Kinect. Our final goal is
to build a 3D map of the world using the point cloud information that comes from the Kinect. To do that, we
will estimate rigid body transformations between positions of the camera, taking advantage of the depth information
made available by the Kinect.

\section{Technical details}
This project will be mostly implemented mostly in Java, so that we can use the April linear algebra libraries, and because our members are more familiar with java.  We will use the Java Native Interface (JNI) to allow us to implement some of the computer vision components in C++, specifically so that we can use OpenCV to extract and match SIFT features.

	We will initially limit our selves at first to 3 DOF system, translation in the x-y plane and yaw.  With this simpler system we will initially implement Visual Odometry approach to compute the Rigid Body transformation between each successive frame. This will simplify the implementation and get something working much more quickly.   We can then extend this to a 6 DOF system with x,y,z, roll, pitch and yaw once we get the simpler system working.
	
We will use Vis, a java visualization tool written by the April Lab to render and explore our three dimensional map.   Rather than simply storing and rendering the point cloud directly which would result in a representation that was constantly expanding as the system ran longer, we intend to use a voxel representation which should reduce the memory requirements and hopefully more fluid representation of the world.  We still need to decide on the resolution of the voxels but we are probably looking at 5 cubic centimeters.  Then when we receive particle cloud information, we will simply use this particle cloud to update the voxels, rather than displaying the cloud directly.   To update the voxels, we will project the particle cloud into our voxel space, and then for each particle in the cloud see which voxel that particle falls into and then update the color of the voxel with the particle information.  We could also use opacity as a way of reflecting our certainty that there is a solid object at that location. 

\section{Milestones achieved}
- plan of how to achieve objective - i.e. cheating with planar motion and only visual odometry; possible extending that to 6DOF full SLAM if we have time (make sure to note that 1 month is not enough to do full Visual SLAM according to Prof. Olson)

- reading quite a few papers and settling on one particular method (dieter fox's)

\section{Milestones remaining}
- stuff from the previous document

\end{document}