\documentclass[12pt]{article}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath, algorithmic, color, multicol}
\usepackage{subfigure}


\title{Progress Report}
\author{
	John Peterson (jrpeters)\\
	Paridhi Desai (paridhid)\\
	Pedro d'Aquino (pdaquino)}

\begin{document}
\maketitle

\section{Introduction}
For mobile robots to be useful in the real world they must be able to move through the world 
with the same ease and proficiency that the humans they are assisting do.  To navigate the world, 
mobile robots must first localize and map their surroundings, in s step called SLAM, before they are 
able to apply other path finding algorithms to choose their actions.   Unfortunately SLAM usually relies 
on expensive and numerous range sensors which significantly raise the cost of any mobile robot. Cameras
offer an interesting solution to this problem.  They are significantly cheaper and more versatile than the 
sensors customarily used in SLAM applications, such as laser scanners.

For this project, we will investigate solving the SLAM problem using a Microsoft Kinect. Our final goal is
to build a 3D map of the world using the point cloud information that comes from the Kinect. To do that, we
will estimate rigid body transformations between positions of the camera, taking advantage of the depth information
made available by the Kinect.

\section{Technical details}
The underlying technical problem we will solve in this project is how to compute rigid body transformations (RBTs) between camera poses. There are, roughly speaking, two kinds of situations in which we are interested. The first is to compute the RBT between two sequential camera frames. Solving this will allow us to implement a visual odometry technique. The second situation is loop closures. A loop closure occurs when an area that has already been visited by the camera is seen again, potentially from a very different viewpoint. Detecting loop closures is substantially more difficult, but an essential aspect of a full fledged SLAM system.

\subsection{Frame matching}
We will base our approach on ``RGB-D Mapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments" by Peter Henery et al. In that paper, and in our project as well, loop closures and visual odometry are solved in almost identical ways. The idea is to use SIFT features and point cloud information to compute the RBT alignment between two frames.

An initial approximation for the RBT is obtained by looking at SIFT features in both frames. We will use RANSAC to determine feature correspondances the frames.  For each RANSAC iteration, we will select SIFT features randomly from both images, and compute the optimal rigid body transformation between the two frames.  We will then apply this RBT to the point cloud and compute a consensus score based on how many points fall into occupied voxels in our world representation (more on that below).  The RBT that yields the highest consensus is selected.

RANSAC operates on SIFT features detected on both frames. Our second method for matching frames opperates on the point cloud provided by the Kinect. The algorithm is called iterative closest point, or ICP. For each iteration, it computes the RBT that aligns each point on the first cloud with the closest point on the other cloud. One important aspect of ICP is that it is prone to local minima if the initial clouds are too distant. Therefore, we use the RBT found through RANSAC and apply it to the point clouds, in order to give a good initial approximation on which ICP can be more reliable.

The final RBT is obtained by weighting the transformation computed through RANSAC and the one that came from ICP. The weight factor will be determined empirically.

We should note that several optimizations can be implemented. For instance,  to accelerate RANSAC we can use depth information, color information, and additional heuristics to increase our likelihood of selecting corresponding features. We will also be able to improve the accuracy of ICP by taking into account the color data tied to each of the voxels and each of the particles from the point cloud. 

\subsection{Visualization}
To map and visualize the surroundings we will use Vis, a Java visualization library written by the April Lab to render and explore our three dimensional map. Rather than simply storing and rendering the point cloud directly, we intend to use a voxel representation.  This allows us to avoid rapidly increasing memory requirements as more and particles are acquired, since rather than being added directly to the map, these particles can be incorporated into the voxel representation.  This representation has benefits for our earlier RANSAC, which would make use of three dimensional occupancy grids anyway, and it also provides a convinient way to visualize our map.

To update the voxels, we will project the particle cloud into our voxel space, and then for each particle in the cloud see which voxel that particle falls into and then update the color of the voxel with the particle information.  We may also use opacity as a way of reflecting our certainty that there is a solid object at that location which provides a mechanism for us to deal with the noisy information provided by the Kinect.  This voxel representation is a compromise beween the the straight point cloud that the Kinect provides and the surfel representation illustrated in ``RGB-D Mapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments" by Peter Henery et al. This surfel representation allowed them to construct a very acurate map of the environment within memory constraints, however this representation slowed down their algorithm significantly and would likely be a project in and of itself to implement.  We hope that the voxel representation will provide the rendering framework that we need to move the project forwards without being too difficult to implement.

\subsection{Subgoals}

% XXX explain that full slam in one month is actually fucking impossible

We will initially limit ourselves at first to 3 DOF system, translation in the x-y plane and yaw only.  With this simpler system we will initially implement mapping based only on the visual odometry computed above completely neglecting loop closures.  After this system has been successfuly implemented we will move on to performing loop closures using both the SIFT features we are gathering as well as planar scan matching.  Finally we will extend this to a full 6 DOF system with x,y,z, roll, pitch and yaw which will allow us to map arbitrary motions.  

This project will be mostly implemented in Java, so that we can use Professor Olson's April laboratory linear algebra libraries, and because our members are more familiar with Java.  We will use Java Native Interface (JNI) to allow us to implement some of the computer vision components in C++, specifically so that we can use OpenCV to extract and match SIFT features. 


\section{Milestones achieved}
- reading quite a few papers and settling on one particular method (dieter fox's)

 - plan of how to achieve objective - i.e. cheating with planar motion and only visual odometry; possible extending that to 6DOF full SLAM if we have time (make sure to note that 1 month is not enough to do full Visual SLAM according to Prof. Olson)

- Acquiring and ploting uncalibrated point cloud data using Vis on linux.

- Decided on World representation using Voxels.

- General Planning

\section{Milestones remaining}
Calibrate Point Cloud: Map depth information to pixels to generate an accurate point cloud.

Extract Sift Features

Construct Rigid Body Transformation for transforming Point Cloud between image frames

Set up JNI to pass SIFT features to Java side

Construct Voxel World Representation: transforms point cloud information to voxels

Determine Feature Corespondances using RANSAC to get Rigid Body Transformation 

Loop Closure using sparse SIFT features and point cloud matching

Extend to 6 Degrees of Freedom


\end{document}