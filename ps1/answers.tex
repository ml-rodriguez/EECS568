\title{Answers to Problem Set 1}
\author{
				Lauren Hinkle (lhinkle)\\
        Pedro d'Aquino  (pdaquino)\\				
				Robert Goeddel (rgoeddel)\\
        Yash Chitalia (yashc)}
\date{\today}

\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}
\maketitle

\section{Task 1: Linear Algebra Review}

\paragraph{A}

\paragraph{B}
\begin{equation}
A = \left( \begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \end{array} \right)
\end{equation}

\paragraph{C}
The idea is that $AA^T = I \Rightarrow A^TA=I$ only if $A$ is square. We proved it by showing that if $A$ is rectangular it only works one way around (i.e. if $AA^T=1$ then $A^TA \neq I$).

\paragraph{D}

\section{Task 2: Multiavariate Gaussians}

\paragraph{A}
Let $N$ be the number of samples, $K$ be the number of random variables in $\mathbf{x}$ and $\mathbf{\mu}$ be the sample mean vector. The $i$-th sample of the random variable $\mathbf{x}_k$ is denoted by $\mathbf{x}_{ik}$ Then each element of $\mathbf{\mu}$ is going to be of the form:

\begin{equation}
\mu_k = \frac{1}{N}\displaystyle\sum_{i=1}^{N}{x_{ik}}, k=1,2,...,K
\end{equation}

And every element $\sigma_{jk}$ of the $K \times K$ covariance matrix $\Sigma$ will be given by:

\begin{equation}
\sigma_{jk}=\frac{1}{N-1}\displaystyle\sum_{i=1}^{N}{(x_{ij}-\mu_j)(x_{ik}-\mu_{k})}
\end{equation}

\paragraph{B}

We first prove that for any $m \times n$ matrix $A$, $AA^T$ is SPD. Let $x$ be any column vector of dimensions $m\times1$ and let

\[
\underbrace{x^T}_{1\times m}\underbrace{A}_{m\times n}=\underbrace{t}_{1\times n}
\]

Then:

\[
x^TAA^Tx = \underbrace{t}_{1\times n}\underbrace{t^T}_{n \times 1} = \underbrace{s}_{1\times 1}
\]

Additionaly, $s\geq 0$:

\[s = \displaystyle\sum_{i=1}^n{t_i^2}\]

Therefore, $AA^T$ is SPD. We can extend this to prove that any covariance matrix is also SPD. Again, let$x$ be any column vector of dimensions $m\times1$. We need to prove that $x^T\Sigma x \geq 0$.

\[
x^T\Sigma x=x^TE\left[\left(X-E[X]\right)\left(X-E[X]\right)^T\right]x
\]

Let $A=X-E[X]$. Then we rewrite the equation as:

\[
x^T\Sigma x=x^TE\left[AA^T\right]x = E\left[x^TAA^Tx\right]
\]

But we have already proved that $x^TAA^Tx$ is a scalar $s$, $s \geq 0$:

\[
x^T\Sigma x  =E\left[s\right] \geq 0
\]

\paragraph{C}
Let $Z$ be a multivariate distribution comprised of independent standard normal univariate distributions. Then the mean $\mu_Z = 0$ and the covariance $\Sigma_Z=I$, the identity matrix. If we apply a linear transformation of the form $AZ + b$, we will have a multivariate normal distribution of the form:

\[
\mathcal{N}\left(A\mu_Z + b, A\Sigma_Z A^T\right) = \mathcal{N}\left(A0 + b, BIB^T\right) = \mathcal{N}\left(b, AA^T\right)
\]

Therefore, we can draw from $\mathcal{N}\left(\mu,P\right)$ by making $b=\mu$ and $AA^T=P$ (using, for instance, Cholesky decomposition).

\paragraph{D}
Let there be a two-dimensional distribution with given covariance $\Sigma$ such that there is a $\chi^2$ error of $c$ for some point $p$ where $p = \mu +
\left[ {\begin{smallmatrix}
\alpha cos(\theta)  \\
\alpha sin(\theta)  \\
 \end{smallmatrix} } \right]$
with a given value of $\theta$ and unknown $\alpha$.

By definition, $\chi^2 = \mathbf{w}^T \Sigma^{-1} \mathbf{w}$ where $\mathbf{w} = \mathbf{p}-\mathbf{\mu}$.  Then 
\begin{align*}
\mathbf{w} &= \mu +
	\left[ {\begin{smallmatrix}
	\alpha cos(\theta)  \\
	\alpha sin(\theta)  \\
 	\end{smallmatrix} } \right] 
	- \mu \\
 &= \alpha \left[ {\begin{smallmatrix}
	cos(\theta)  \\
	sin(\theta)  \\
	 \end{smallmatrix} } \right]
\end{align*}
And therefore the unknown $\alpha$ can be calculated.
\begin{align*}
\chi^2 = c &=  \mathbf{w}^T \Sigma^{-1} \mathbf{w} \\
&=  \alpha \left[ {\begin{smallmatrix}
	cos(\theta)  sin(\theta)  \\
	 \end{smallmatrix} } \right]
 	\Sigma
 	\alpha \left[ {\begin{smallmatrix}
	cos(\theta)  \\
	sin(\theta)  \\
	 \end{smallmatrix} } \right] \\
&= \alpha^2 \left[ {\begin{smallmatrix}
	cos(\theta)  sin(\theta)  \\
	 \end{smallmatrix} } \right]
 	\Sigma
 	\left[ {\begin{smallmatrix}
	cos(\theta)  \\
	sin(\theta)  \\
	 \end{smallmatrix} } \right] \\
\alpha^2 &= \frac{c}{\left[ {\begin{smallmatrix}
	cos(\theta)  sin(\theta)  \\
	 \end{smallmatrix} } \right]
 	\Sigma
 	\left[ {\begin{smallmatrix}
	cos(\theta)  \\
	sin(\theta)  \\
	 \end{smallmatrix} } \right]} \\
\alpha &=\sqrt{ c \left( \left[ {\begin{smallmatrix}
	cos(\theta)  sin(\theta)  \\
	 \end{smallmatrix} } \right]
 	\Sigma
 	\left[ {\begin{smallmatrix}
	cos(\theta)  \\
	sin(\theta)  \\
	 \end{smallmatrix} } \right]
	 \right)^{-1}}
\end{align*}


\paragraph{E}

\paragraph{F}

\section{Task 3: Covariance Matrices and Covariance Projection}

\paragraph{A}

\paragraph{B}


\end{document}
