\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath, algorithmic, color}


\title{Answers to Problem Set 1}
\author{
	Lauren Hinkle (lhinkle)\\
        Pedro d'Aquino  (pdaquino)\\
	Robert Goeddel (rgoeddel)}

\begin{document}
\maketitle
\pagebreak

\section{Graphical Models and Conditional Distributions}

\paragraph{A}
\begin{enumerate}
	\item $u_i$ and $u_{i+1}$ are unconditionally independent, but are dependent given the robot trajectory.
	\item $z_i$ and $z_{i+1}$ are dependent, even given all the positions, because the dependence flows through $L_0$.
	\item If the trajectory of the robot is known, the our posterior estimate of the locations of the
    landmarks is only dependent on the landmark observation noise. For example, if the noise is Gaussian,
    we'd expect the mean of our observations of a particular landmark to converge on the actual location
    of the landmark as we gain more observations. % XXX
\end{enumerate}

\paragraph{B}
\begin{enumerate}
	% Part B1
	\item  Given a binary variable $x\in\{a,b\}$, after one observation $z_1: p(x = a | z_1)$, the probability that $x=a$ in terms of only $p(a)$, $p(z_1| a)$ and $p(z_1 | b)$ is
	\begin{align*}
	p(a|z_1) &= \frac{p(z_1|a)p(a)}{p(z_1)} \\
						&= \frac{p(z_1|a)p(a)}{p(z_1|a)p(a) + p(z_1|b)p(b)} \\
						&= \frac{p(z_1|a)p(a)}{p(z_1|a)p(a) + p(z_1|b)(1-p(a))}
	\end{align*}
	% Part B2 XXX - not sure about p(z_1|z_2), also doesn't align with results in B3
	\item  First we show $p(z_1,z_2|a) = p(z_2|a,z_1)p(z_1|a)p(a)$:
	\begin{align*}
	p(z_1,z_2|a) &= \frac{p(z_1,z_2,a)}{p(a)} \\
			     &= \frac{p(z_2|a,z_1)p(a,z_1)}{p(a)} \\ 
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(a)}\\
			     &=p(z_2|a,z_1)p(z_1|a)p(a)
	\end{align*}
	Then the correct expression for the probability of $x$ after two Bayes updates is derived as follows:
	\begin{align*}
	p(a|z_1,z_2) &= \frac{p(z_1,z_2|a)p(a)}{p(z_1,z_2)} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)}{p(z_1,z_2)} \\ 
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1)p(z_2)}\\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{(p(z_1|a)p(a)+p(z_1|b)p(b))\cdot(p(z_2|a)p(a)+p(z_2|b)p(b))}
	\end{align*}
	This last step can be done because $z_1$ and $z_2$ are independent.  David's mistake occurs when he assumes the independent observations $z_1$ and $z_2$ have equal probability:
	\begin{align*}
	p(a|z_1,z_2) &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1)p(z_2)} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1)^2} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{(p(z_1|a)p(a)+p(z_1|b)p(b))^2}
	\end{align*}
	% Part B3
	\item  Let $p(z | a) = p(z | b)$.
	\begin{align*}
	p(a,z_1,z_2,z_3,z_4) &= p(a,z_1,z_2,z_3|z_4)p(z_4) \\
					  &=  p(a,z_1,z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
					  &=  p(a,z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
					  &=  p(a|z_1,z_2,z_3,z_4)p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
	p(a|z_1,z_2,z_3,z_4) &=	\frac{p(a,z_1,z_2,z_3,z_4)}{p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4)} \\
	 				  &=  \frac{p(z_4|a,z_1,z_2,z_3)p(z_3|a,z_1,z_2)p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4)}
	\end{align*}

    \begin{align*}
    p(z) &= p(z|a)p(a) + p(z|b)p(b) \\
        &= p(z|a)(p(a) + p(b)) \\
        &= p(z|a) \\
    p(a|z) &= \frac{p(z|a)p(a)}{p(z)} \\
        &= \frac{p(z|a)p(a)}{p(z|a)} \\
        &= p(a) \\
    \end{align*}

    From this we claim that $a$ is independent of observations $z$. Thus, given
    any number of observations $z$, we learn no more about $a$, so:

    $p(a|z_1,z_2,z_3,z_4) = p(a)$
\end{enumerate}

\pagebreak

\section{Matrix Sparsity}

% XXX Assumed reflexive edges? YES / NO?
\paragraph{(i)}
The adjaceny matrix for the given graph looks like:

\begin{tabular}{| c || c | c | c | c | c | c | c | c |}
\hline
& a & b & c & d & e & f & g & h \\
\hline \hline
a & & X & X & & & & & X \\
\hline
b & X & & X & & & & & \\
\hline
c & X & X & & X & X & & & \\
\hline
d & & & X & & X & & & \\
\hline
e & & & X & X & & X & & \\
\hline
f & & & & & X & & & X  \\
\hline
g & & & & & & & & X \\
\hline
h & X & & & & & X & X & \\
\hline
\end{tabular}

\paragraph{(ii)}
Node C has the highest degree with 4 connections. Marginalizing out C gives us:

\begin{tabular}{| c || c | c | c | c | c | c | c |}
\hline
& a & b & d & e & f & g & h \\
\hline \hline
a & & X & X & X & & & X \\
\hline
b & X & & X & X & & & \\
\hline
d & X & X & & X & & & \\
\hline
e & X & X & X & & X & & \\
\hline
f & & & & X & & & X  \\
\hline
g & & & & & & & X \\
\hline
h & X & & & & X & X & \\
\hline
\end{tabular}

\paragraph{(iii)}
g, b, a, d, c, e, f, h

\paragraph{(iv)}
\emph{XXX Ask about this...we must be answering what they MEANT to say}
Given graph $G = (V, E)$, let the number of vertices $|V| = n$ and number of
edges $|E| = m$.

First, find the minimum degree node. Simple bookkeeping during graph
construction allows us to find minimum degree in constant time, so this
process only takes $\mathcal{O}(n)$.

Given this minimum degree node, now we must marginalize it. This means building
a clique between every node connected by an edge to the minimum degree node and
then removing said node. Up to $n-1$ nodes may be connected to the min degree
node and we need to check the connnections between all of these. In case 1, an
edge already exists and we don't do anything. This takes constant time. In case
2, we need to create the edge. Again, this takes constant time (add 2 entries
to the adjacency matrix, increment degree counters). Therefore, the entire
process takes $\mathcal{O}(n^2)$ time.

We repeat this process $n-1$ times in total to calculate the minimum degree
variable elimination ordering, so we get $\mathcal{O}(n)\mathcal{O}(n + n^2)
= \mathcal{O}(n^3)$ worst case time.

Clever trick: Since we know that we chose the node of min degree, we know it has
no more than $\frac{m}{n}$ connected edges. Thus the complexity ends up looking
like $\mathcal{O}(n^2 + \frac{m^2}{n})$. Analysis gets tricky here, though,
since marginalizing a bad node can actually result in an increase in edges. We
do not yet have proof that this behavior does not affect the asymptotic
complexity.

\paragraph{(v)}
Instead of actually finding the minimum degree node every time, just calculate
the minimum degrees once at the beginning and then sort the list. Use this list
as an approximation of the actual one. Instead of taking $\mathcal{O}(n^2)$
operations to calculate the next minimum degree node, we spend
$\mathcal{O}(n\log(n))$ operations once to approximate the ordering. The total
time to marginalize the graph is still $\mathcal{O}(n^3)$.

\pagebreak
\section{SLAM}

\subsection*{A}

\subsubsection*{Observation and Motion Models}
\textcolor{red}{observation and motion models and the analytic Jacobians for them}

\subsubsection*{Rank Deficiency}
The information matrix is inevitably rank deficient.  To deal with this rank deficiency, we employed two methods.  We used a constraint edge to pin the initial the first observation.  Additionally, we used Tikhonov regularization on the information matrix to calculate 
$$\Delta x = (J^T \Sigma^{-1}J + \Gamma^T \Gamma)^{-1}J^T \Sigma^{-1} r$$
where $\Gamma = \alpha I$ with a scaling value $\alpha = 1000$.  The value for $\alpha$ was chosen to be a small percentage of the size of the values in our weight matrix, which varied between several and a hundred thousand.


\subsubsection*{Visualizing Least Squares SLAM}
\textcolor{red}{Provide screen shots for the trajectory versus ground truth.}

\subsubsection*{Considering $\chi^2$}
$$\chi^2 = \Delta x^TJ^T\Sigma^{-1}J\Delta x - 2\Delta x^T\Sigma^{-1}r + r^T\Sigma^{-1}r$$
When the least squares optimization has a set number of iterations at every time stamp it is unnecessarily slow.  When no new landmark observations have been made the $\chi^2$ error barely changes, and so performing many iterations has very little change in the quality of the line.  To help mitigate this, we use the $\chi^2$ error to help determine how many iterations of least squares optimization to run at each time stamp.  When the change in $\chi^2$ from one iteration to the next is less than a given threshold, the solution is accepted.  After testing various threshold, we settled on $0.005$, which seemed to provide a reasonable compromise between allowing the program to run until it is not changing very much, and not being too slow.  Additionally, to help speed up the runtime, we used a maximum of 100 iterations.

\textcolor{red}{Plot of chi2 error as function of the number of iterations performed}


\subsection*{B}

For this part, the robot was not given the ID for each landmark and therefore had to associate landmark observations on its own, using $\chi^2$ nearest neighbors.  

\subsubsection*{Optimization Proposals}
\textcolor{red}{propose several different approaches to mitigate the slowness problem . For example, you might consider how and when to skip some computations, or how to approximate the posterior computation (and determine when such an approximation is acceptable.}

\subsubsection*{Our $\chi^2$ Nearest Neighbor Algorithm}
We implemented $\chi^2$ nearest neighbor using both Euclidean distance and by considering the effect on the $\chi^2$ value of the system caused when associating each new landmark observation with each previous landmark observed.  

Euclidean distance was used to minimize the number of possible landmark associations that would be considered.  In addition to not needing to consider other landmarks seen at the same observation time as possible associates, any landmark that was outside a threshold radius of the suspected landmark location was ignored.  Since the $\chi^2$ nearest neighbor is more complex than calculating Euclidean distance, this is intended to shorten the total computation time.  The threshold \textcolor{red}{$XXX$} was chosen to still allow a large area around the suspected location of the landmark to increase the likelihood of finding the correct association, should the robot's perceived self-location deviate greatly from its actual location.

Each previously observed landmark within the given area is considered for association using $\chi^2$ nearest neighbor.  Each association is considered to be correct, and ten iterations of the Least-Squares Optimization loop (created in part A) are performed.  The resulting $\chi^2$ values are calculated.  We chose to do ten iterations because even very large initial $\chi^2$ begin to converge within ten iterations, although they may not reach their minimum.  Iterating further, however, would lead to time-consuming steps.  If none of the resulting $\chi^2$ fall within a given difference threshold of the current $\chi^2$ of the system, then the new landmark observation is turned into a new landmark.  Otherwise, it is associated with the landmark that results in the lowest $\chi^2$ value.  The threshold we chose for change in $\chi^2$ is \textcolor{red}{$20\%$} of the previous $\chi^2$ value.


%\begin{algorithmic}
%\STATE{\bf Associate Landmarks}
%\STATE{\bf input:} list $newLandmarksObservations$
%\STATE{\bf output:} list $landmarksObserved$
%\FOR {every landmark in $newLandmarksObservations$}
%	\FOR{every  landmark in $previousLandmarksObservations$}
%		\IF{$distance(newLandmark, oldLandmark) < DIST\_THRESHOLD$}
%			\STATE Associate landmark and new landmark observation
%			\STATE Do 10 least-squares optimization iterations
%			\IF{$new\_chi^2 < max(CHI\_THRESHOLD*old\_chi^2, old\_ch1^2 + 1)$}
%				\STATE add $oldLandmark$ to $matchedList$
%			\ENDIF
%		\ENDIF
%	\ENDFOR
%	\IF{$matchedList$ is empty}
%		\STATE add $newLandmark$ to $landmarksObserved$
%	\ELSE
%		\STATE add $minChi^2(matchedList)$ to $landmarksObserved$
%	\ENDIF
%\ENDFOR
%\end{algorithmic}

\subsubsection*{Error Analysis}

\textcolor{red}{Put both errors in landmark associations and surface plot showing the number of data association errors as a function of odometry and measurement noise here.}




\end{document}
