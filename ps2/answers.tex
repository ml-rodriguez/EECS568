\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}

\title{Answers to Problem Set 1}
\author{
				Lauren Hinkle (lhinkle)\\
        Pedro d'Aquino  (pdaquino)\\
				Robert Goeddel (rgoeddel)}

\begin{document}
\maketitle
\pagebreak

\section{Graphical Models and Conditional Distributions}

\paragraph{A}
\begin{enumerate}
	\item $u_i$ and $u_{i+1}$ are unconditionally independent, but are dependent given the robot trajectory.
	\item $z_i$ and $z_{i+1}$ are dependent, even given all the positions, because the dependence flows through $L_0$.
	\item If the trajectory of the robot is known, the our posterior estimate of the locations of the
    landmarks is only dependent on the landmark observation noise. For example, if the noise is Gaussian,
    we'd expect the mean of our observations of a particular landmark to converge on the actual location
    of the landmark as we gain more observations. % XXX
\end{enumerate}

\paragraph{B}
\begin{enumerate}
	% Part B1
	\item  Given a binary variable $x\in\{a,b\}$, after one observation $z_1: p(x = a | z_1)$, the probability that $x=a$ in terms of only $p(a)$, $p(z_1| a)$ and $p(z_1 | b)$ is
	\begin{align*}
	p(a|z_1) &= \frac{p(z_1|a)p(a)}{p(z_1)} \\
						&= \frac{p(z_1|a)p(a)}{p(z_1|a)p(a) + p(z_1|b)p(b)} \\
						&= \frac{p(z_1|a)p(a)}{p(z_1|a)p(a) + p(z_1|b)(1-p(a))}
	\end{align*}
	% Part B2 XXX - not sure about p(z_1|z_2), also doesn't align with results in B3
	\item  The correct expression for the probability of $x$ after two Bayes updates is derived as follows:
	\begin{align*}
	p(a|z_1,z_2) &= \frac{p(a,z_1|z_2)}{p(z_1,z_2)} \\
			     &= \frac{p(z_2|a,z_1)p(a,z_1)}{p(z_1,z_2)p(z_2)} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1|z_2)p(z_2)^2} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_2|z_1)p(z_1)p(z_2)}
	\end{align*}
	David's mistake occurred in the first line of the derivation, when he assumed $p(a|z_1,z_2) = \frac{p(a,z_1|z_2)}{p(z_1)}$.  This mistake led to his incorrect derivation:
	\begin{align*}
	p(a|z_1,z_2) &= \frac{p(a,z_1|z_2)}{p(z_1)} \\
			     &= \frac{p(z_2|a,z_1)p(a,z_1)}{p(z_1)} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1)^2} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{(p(z_1|a)p(a)+p(z_1|b)p(b))^2}
	\end{align*}
	% Part B3
	\item  Let $p(z | a) = p(z | b)$.
	\begin{align*}
	p(a,z_1,z_2,z_3,z_4) &= p(a,z_1,z_2,z_3|z_4)p(z_4) \\
					  &=  p(a,z_1,z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
					  &=  p(a,z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
					  &=  p(a|z_1,z_2,z_3,z_4)p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
	p(a|z_1,z_2,z_3,z_4) &=	\frac{p(a,z_1,z_2,z_3,z_4)}{p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4)} \\
	 				  &=  \frac{p(z_4|a,z_1,z_2,z_3)p(z_3|a,z_1,z_2)p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4)}
	\end{align*}

    \begin{align*}
    p(z) &= p(z|a)p(a) + p(z|b)p(b) \\
        &= p(z|a)(p(a) + p(b)) \\
        &= p(z|a) \\
    p(a|z) &= \frac{p(z|a)p(a)}{p(z)} \\
        &= \frac{p(z|a)p(a)}{p(z|a)} \\
        &= p(a) \\
    \end{align*}

    From this we claim that $a$ is independent of observations $z$. Thus, given
    any number of observations $z$, we learn no more about $a$, so:

    $p(a|z_1,z_2,z_3,z_4) = p(a)$
\end{enumerate}

\pagebreak

\section{Matrix Sparsity}

% XXX Assumed reflexive edges? YES / NO?
\paragraph{(i)}
The adjaceny matrix for the given graph looks like:

\begin{tabular}{| c || c | c | c | c | c | c | c | c |}
\hline
& a & b & c & d & e & f & g & h \\
\hline \hline
a & & X & X & & & & & X \\
\hline
b & X & & X & & & & & \\
\hline
c & X & X & & X & X & & & \\
\hline
d & & & X & & X & & & \\
\hline
e & & & X & X & & X & & \\
\hline
f & & & & & X & & & X  \\
\hline
g & & & & & & & & X \\
\hline
h & X & & & & & X & X & \\
\hline
\end{tabular}

\paragraph{(ii)}
Node C has the highest degree with 4 connections. Marginalizing out C gives us:

\begin{tabular}{| c || c | c | c | c | c | c | c |}
\hline
& a & b & d & e & f & g & h \\
\hline \hline
a & & X & X & X & & & X \\
\hline
b & X & & X & X & & & \\
\hline
d & X & X & & X & & & \\
\hline
e & X & X & X & & X & & \\
\hline
f & & & & X & & & X  \\
\hline
g & & & & & & & X \\
\hline
h & X & & & & X & X & \\
\hline
\end{tabular}

\paragraph{(iii)}
g, b, a, d, c, e, f, h

\paragraph{(iv)}
\emph{XXX Ask about this...we must be answering what they MEANT to say}
Given graph $G = (V, E)$, let the number of vertices $|V| = n$ and number of
edges $|E| = m$.

First, find the minimum degree node. Simple bookkeeping during graph
construction allows us to find minimum degree in constant time, so this
process only takes $\mathcal{O}(n)$.

Given this minimum degree node, now we must marginalize it. This means building
a clique between every node connected by an edge to the minimum degree node and
then removing said node. Up to $n-1$ nodes may be connected to the min degree
node and we need to check the connnections between all of these. In case 1, an
edge already exists and we don't do anything. This takes constant time. In case
2, we need to create the edge. Again, this takes constant time (add 2 entries
to the adjacency matrix, increment degree counters). Therefore, the entire
process takes $\mathcal{O}(n^2)$ time.

We repeat this process $n-1$ times in total to calculate the minimum degree
variable elimination ordering, so we get $\mathcal{O}(n)\mathcal{O}(n + n^2)
= \mathcal{O}(n^3)$ worst case time.

Clever trick: Since we know that we chose the node of min degree, we know it has
no more than $\frac{m}{n}$ connected edges. Thus the complexity ends up looking
like $\mathcal{O}(n^2 + \frac{m^2}{n})$. Analysis gets tricky here, though,
since marginalizing a bad node can actually result in an increase in edges. We
do not yet have proof that this behavior does not affect the asymptotic
complexity.

\paragraph{(v)}
Instead of actually finding the minimum degree node every time, just calculate
the minimum degrees once at the beginning and then sort the list. Use this list
as an approximation of the actual one. Instead of taking $\mathcal{O}(n^2)$
operations to calculate the next minimum degree node, we spend
$\mathcal{O}(n\log(n))$ operations once to approximate the ordering. The total
time to marginalize the graph is still $\mathcal{O}(n^3)$.

\pagebreak
\section{SLAM}


\end{document}
