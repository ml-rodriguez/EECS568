\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath, algorithmic, color}


\title{Answers to Problem Set 1}
\author{
	Lauren Hinkle (lhinkle)\\
	Pedro d'Aquino  (pdaquino)\\
	Robert Goeddel (rgoeddel)}

\begin{document}
\maketitle
\pagebreak

\section{Graphical Models and Conditional Distributions}

\paragraph{A}
\begin{enumerate}
	\item $u_i$ and $u_{i+1}$ are unconditionally independent, but are dependent given the robot trajectory.
	\item $z_i$ and $z_{i+1}$ are dependent, even given all the positions, because the dependence flows through $L_0$.
	\item If the trajectory of the robot is known, the our posterior estimate of the locations of the
    landmarks is only dependent on the landmark observation noise. For example, if the noise is Gaussian,
    we'd expect the mean of our observations of a particular landmark to converge on the actual location
    of the landmark as we gain more observations. % XXX
\end{enumerate}

\paragraph{B}
\begin{enumerate}
	% Part B1
	\item  Given a binary variable $x\in\{a,b\}$, after one observation $z_1: p(x = a | z_1)$, the probability that $x=a$ in terms of only $p(a)$, $p(z_1| a)$ and $p(z_1 | b)$ is
	\begin{align*}
	p(a|z_1) &= \frac{p(z_1|a)p(a)}{p(z_1)} \\
						&= \frac{p(z_1|a)p(a)}{p(z_1|a)p(a) + p(z_1|b)p(b)} \\
						&= \frac{p(z_1|a)p(a)}{p(z_1|a)p(a) + p(z_1|b)(1-p(a))}
	\end{align*}
	% Part B2 XXX - not sure about p(z_1|z_2), also doesn't align with results in B3
	\item  First we show $p(z_1,z_2|a) = p(z_2|a,z_1)p(z_1|a)p(a)$:
	\begin{align*}
	p(z_1,z_2|a) &= \frac{p(z_1,z_2,a)}{p(a)} \\
			     &= \frac{p(z_2|a,z_1)p(a,z_1)}{p(a)} \\ 
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(a)}\\
			     &=p(z_2|a,z_1)p(z_1|a)p(a)
	\end{align*}
	Then the correct expression for the probability of $x$ after two Bayes updates is derived as follows:
	\begin{align*}
	p(a|z_1,z_2) &= \frac{p(z_1,z_2|a)p(a)}{p(z_1,z_2)} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)}{p(z_1,z_2)} \\ 
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1)p(z_2)}\\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{(p(z_1|a)p(a)+p(z_1|b)p(b))\cdot(p(z_2|a)p(a)+p(z_2|b)p(b))}
	\end{align*}
	This last step can be done because $z_1$ and $z_2$ are independent.  David's mistake occurs when he assumes the independent observations $z_1$ and $z_2$ have equal probability:
	\begin{align*}
	p(a|z_1,z_2) &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1)p(z_2)} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1)^2} \\
			     &= \frac{p(z_2|a,z_1)p(z_1|a)p(a)}{(p(z_1|a)p(a)+p(z_1|b)p(b))^2}
	\end{align*}
	% Part B3
	\item  Let $p(z | a) = p(z | b)$.
	\begin{align*}
	p(a,z_1,z_2,z_3,z_4) &= p(a,z_1,z_2,z_3|z_4)p(z_4) \\
					  &=  p(a,z_1,z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
					  &=  p(a,z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
					  &=  p(a|z_1,z_2,z_3,z_4)p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4) \\
	p(a|z_1,z_2,z_3,z_4) &=	\frac{p(a,z_1,z_2,z_3,z_4)}{p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4)} \\
	 				  &=  \frac{p(z_4|a,z_1,z_2,z_3)p(z_3|a,z_1,z_2)p(z_2|a,z_1)p(z_1|a)p(a)}{p(z_1|z_2,z_3,z_4)p(z_2|z_3,z_4)p(z_3|z_4)p(z_4)}
	\end{align*}

    \begin{align*}
    p(z) &= p(z|a)p(a) + p(z|b)p(b) \\
        &= p(z|a)(p(a) + p(b)) \\
        &= p(z|a) \\
    p(a|z) &= \frac{p(z|a)p(a)}{p(z)} \\
        &= \frac{p(z|a)p(a)}{p(z|a)} \\
        &= p(a) \\
    \end{align*}

    From this we claim that $a$ is independent of observations $z$. Thus, given
    any number of observations $z$, we learn no more about $a$, so:

    $p(a|z_1,z_2,z_3,z_4) = p(a)$
\end{enumerate}

%\pagebreak

\section{Matrix Sparsity}

% XXX Assumed reflexive edges? YES / NO?
\paragraph{(i)}
The adjaceny matrix for the given graph looks like:

\begin{tabular}{| c || c | c | c | c | c | c | c | c |}
\hline
& a & b & c & d & e & f & g & h \\
\hline \hline
a & & X & X & & & & & X \\
\hline
b & X & & X & & & & & \\
\hline
c & X & X & & X & X & & & \\
\hline
d & & & X & & X & & & \\
\hline
e & & & X & X & & X & & \\
\hline
f & & & & & X & & & X  \\
\hline
g & & & & & & & & X \\
\hline
h & X & & & & & X & X & \\
\hline
\end{tabular}

\paragraph{(ii)}
Node C has the highest degree with 4 connections. Marginalizing out C gives us:

\begin{tabular}{| c || c | c | c | c | c | c | c |}
\hline
& a & b & d & e & f & g & h \\
\hline \hline
a & & X & X & X & & & X \\
\hline
b & X & & X & X & & & \\
\hline
d & X & X & & X & & & \\
\hline
e & X & X & X & & X & & \\
\hline
f & & & & X & & & X  \\
\hline
g & & & & & & & X \\
\hline
h & X & & & & X & X & \\
\hline
\end{tabular}

\paragraph{(iii)}
g, b, a, d, c, e, f, h

\paragraph{(iv)}
\emph{XXX Ask about this...we must be answering what they MEANT to say}
Given graph $G = (V, E)$, let the number of vertices $|V| = n$ and number of
edges $|E| = m$.

First, find the minimum degree node. Simple bookkeeping during graph
construction allows us to find minimum degree in constant time, so this
process only takes $\mathcal{O}(n)$.

Given this minimum degree node, now we must marginalize it. This means building
a clique between every node connected by an edge to the minimum degree node and
then removing said node. Up to $n-1$ nodes may be connected to the min degree
node and we need to check the connnections between all of these. In case 1, an
edge already exists and we don't do anything. This takes constant time. In case
2, we need to create the edge. Again, this takes constant time (add 2 entries
to the adjacency matrix, increment degree counters). Therefore, the entire
process takes $\mathcal{O}(n^2)$ time.

We repeat this process $n-1$ times in total to calculate the minimum degree
variable elimination ordering, so we get $\mathcal{O}(n)\mathcal{O}(n + n^2)
= \mathcal{O}(n^3)$ worst case time.

Clever trick: Since we know that we chose the node of min degree, we know it has
no more than $\frac{m}{n}$ connected edges. Thus the complexity ends up looking
like $\mathcal{O}(n^2 + \frac{m^2}{n})$. Analysis gets tricky here, though,
since marginalizing a bad node can actually result in an increase in edges. We
do not yet have proof that this behavior does not affect the asymptotic
complexity.

\paragraph{(v)}
Instead of actually finding the minimum degree node every time, just calculate
the minimum degrees once at the beginning and then sort the list. Use this list
as an approximation of the actual one. Instead of taking $\mathcal{O}(n^2)$
operations to calculate the next minimum degree node, we spend
$\mathcal{O}(n\log(n))$ operations once to approximate the ordering. The total
time to marginalize the graph is still $\mathcal{O}(n^3)$.

%\pagebreak
\section{SLAM}

\subsection*{A. Least-Squares-style SLAM}
**NOTE: We did not update our code to the new version with noise changes that Joho made.

\subsubsection*{Observation and Motion Models}
\textcolor{red}{observation and motion models and the analytic Jacobians for them}

\subsubsection*{Rank Deficiency}
The information matrix is inevitably rank deficient.  To deal with this rank deficiency, we used Tikhonov regularization on the information matrix to calculate 
$$\Delta x = (J^T \Sigma^{-1}J + \Gamma^T \Gamma)^{-1}J^T \Sigma^{-1} r$$
where $\Gamma = \alpha I$ with a scaling value $\alpha = 1000$.  The value for $\alpha$ was chosen to be a small percentage of the size of the values in our weight matrix, which range into the tens of thousands.


\subsubsection*{Visualizing Least Squares SLAM}
\textcolor{red}{Provide screen shots for the trajectory versus ground truth.}

Our implementation of Least Squares SLAM draws each of the landmark observation edges and the estimated position of the landmark at each iteration of least squares.  This allows the viewer to watch least squares working. The edges are drawn in different colors that depend on how ``good" they are.  The closer the edge color is to black, the closer the corresponding residual is to 0.  The edge becomes more purple as the residual becomes increasingly negative and more yellow as it becomes increasingly positive.

\subsubsection*{Considering $\chi^2$}
When the least squares optimization has a set number of iterations at every time stamp it is unnecessarily slow.  When no new landmark observations have been made the $\chi^2$ error, which is calculated as  
$$\chi^2 = \Delta x^TJ^T\Sigma^{-1}J\Delta x - 2\Delta x^T\Sigma^{-1}r + r^T\Sigma^{-1}r$$
barely changes, and so performing many iterations has very little change in the quality of the line.

To help mitigate excessively iterating, we use the $\chi^2$ error to help determine how many iterations of least squares optimization to run at each time stamp.  When the change in $\chi^2$ from one iteration to the next is less than a given threshold, the solution is accepted.  After testing various threshold, we settled on $0.005$, which seemed to provide a reasonable compromise between allowing the program to run until it is not changing very much, and not being too slow.  Additionally, to help speed up the runtime, we used a maximum of 100 iterations.

\textcolor{red}{Plot of chi2 error as function of the number of iterations performed}


\subsection*{B. $\chi^2$ Nearest Neighbor}

For this part, the robot was not given the ID for each landmark and therefore had to associate landmark observations on its own, using $\chi^2$ nearest neighbors.  In order to truly test the best associations for each of the new landmark observations, it would be necessary to create a set for every possible set of associations and find the set of associations that produced the lowest $\chi^2$.  A single association between one new observation and an old landmark may have a positive increase in the $\chi^2$ value, but it may cause the other new observations to have such poor associations (or to become new landmarks), and therefore the overall system is worse than if a poorer first $\chi^2$ choice is made.  However, doing this many calculations would be very slow.  We therefore propose methods for mitigating this and share the changes we use in our code.

\subsubsection*{Optimization Proposals}
Nearest neighbors could be optimized by decreasing the number of computations required.  This could be done by decreasing the number of previously seen landmarks each new observations is compared to.  Additionally, the $\chi^2$ resulting from an association could be estimated by performing less Least-Squares iterations. We employed both of these methods in our code.

Another method to decrease computation would be to make assumptions about landmarks seen in sequential observations.  At each time stamp one could consider the landmarks observed in each of the last several odometry positions.  For each of those landmarks, an edge between that landmark and the current position could be created.  If the resulting $\chi^2$ is small, then it could be assumed that the current new observation is the same as the this one.  As with the prev

Another technique would be to use a variant of RANSAC to choose which observations are most likely to be the same landmark using $\chi^2$ estimate.

\subsubsection*{Our $\chi^2$ Nearest Neighbor Algorithm}
In our discussions about associating two landmarks, we determined that making false associations was a greater failure than not making a correct observation.  A few simple tests showed that a single incorrect association could result in a very large error (\textcolor{red}{Do we have an image of this we could reference?}).  As a result, our implementation seeks to be conservative about associating two landmarks.

We implemented $\chi^2$ nearest neighbor using both Euclidean distance and by considering the effect on the $\chi^2$ value of the system caused when associating each new landmark observation with each previous landmark observed.  

Euclidean distance was used to minimize the number of possible landmark associations that would be considered.  Any landmark that was outside a threshold radius of the suspected landmark location was ignored.  Since the $\chi^2$ nearest neighbor is more complex than calculating Euclidean distance, this is intended to decrease the computation time.  The threshold \textcolor{red}{$XXX$} was chosen to still allow a large area around the suspected location of the landmark to increase the likelihood of finding the correct association, should the robot's perceived self-location deviate greatly from its actual location.

Each previously observed landmark within the given area is considered for association using $\chi^2$ nearest neighbor.  Each association pair between a previous landmark and new observations is tested by assuming the association is correct and performing ten iterations of the Least-Squares Optimization loop.  The resulting $\chi^2$ values are calculated.  We chose to do ten iterations because even very large initial $\chi^2$ begin to converge within ten iterations, although they may not reach their minimum, and iterating as much as in part A would lead to time-consuming steps.  The pair with the best resulting $\chi^2$ that falls within a given threshold change from the current $\chi^2$ is a associated.  The next best pair which does not include either of the already used landmarks is then associated (if it falls within the threshold).  This process is continued until all the new observations have either been associated with a previous landmark or there are no landmarks left within the threshold.  Any remaining new observations become new landmark poses.  The threshold we used was \textcolor{red}{$max(1.2*old\chi^2, old\chi^2 + 1$)}.  This allowed $\chi^2$ increments proportional to the current $\chi^2$ value, which grows as more observations are made.

%\begin{algorithmic}   %%This was the old method.  Not sure that we want it anyway.
%\STATE{\bf Associate Landmarks}
%\STATE{\bf input:} list $newLandmarksObservations$
%\STATE{\bf output:} list $landmarksObserved$
%\FOR {every landmark in $newLandmarksObservations$}
%	\FOR{every  landmark in $previousLandmarksObservations$}
%		\IF{$distance(newLandmark, oldLandmark) < DIST\_THRESHOLD$}
%			\STATE Associate landmark and new landmark observation
%			\STATE Do 10 least-squares optimization iterations
%			\IF{$new\_chi^2 < max(CHI\_THRESHOLD*old\_chi^2, old\_ch1^2 + 1)$}
%				\STATE add $oldLandmark$ to $matchedList$
%			\ENDIF
%		\ENDIF
%	\ENDFOR
%	\IF{$matchedList$ is empty}
%		\STATE add $newLandmark$ to $landmarksObserved$
%	\ELSE
%		\STATE add $minChi^2(matchedList)$ to $landmarksObserved$
%	\ENDIF
%\ENDFOR
%\end{algorithmic}

\subsubsection*{Error Analysis}

\textcolor{red}{Put both errors in landmark associations and surface plot showing the number of data association errors as a function of odometry and measurement noise here.}




\end{document}
