\documentclass[12pt]{article}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath, algorithmic, color, multicol}
\usepackage{subfigure}


\title{Answers to Problem Set 3}
\author{
	Lauren Hinkle (lhinkle)\\
	Pedro d'Aquino (pdaquino)\\
	Robert Goeddel (rgoeddel)}

\begin{document}
\maketitle
\pagebreak

%% TASK 1
\section{FastSLAM}

% PART A
\paragraph{A}
Starting with an existing EKF...
\subparagraph{i}
After first observation update, you need to grow the covariance matrix to
be $7 \times 7$, but we don't expect the covariances of the existing matrix
to change. We do \emph{not} apply the normal Kalman gain equations. Instead,
we add rows to $J_x^f$. % XXX What is f?
In this case, we have observations $(r_1, \theta_1)$ that can be projected
into $(x,y)$ space from position $(x_0, y_0, \theta_0)$.
XXX % XXX WARNING, stuff here needs to be written

$K = \Sigma_x J_x^T(J_x \Sigma_x J_x^T + \Sigma_z)^{-1} $ \\
$\Sigma_z = \left[ \begin{array}{c c}
3 & 0 \\
0 & 1 \\
\end{array} \right]$ \\
$x' = x + Kr$ \\
$\Sigma_x' = (I - KJ_x) \Sigma_x$ \\

\subparagraph{ii}
After second observation update (\emph{Note: additional primes denote
    later state}).
$K' = \Sigma_x' J_{x'}^T(J_{x'} \Sigma_x' J_{x'}^T + \Sigma_z)^{-1} $ \\
$x'' = x' + K'r'$ \\
$\Sigma_x'' = (I - K'J_{x'}) \Sigma_x'$ \\

\subparagraph{iii}
After observing $f_1$: \\
$$\mu = \left[ \begin{array}{c}
3 \\
2 \\
pi \\
12 \\
15 \\
3 \\
-8
\end{array}\right],
\Sigma = \left[ \begin{array}{c c c c c c c}
4.00 & 1.00 & 2.00 & 1.00 & 1.00 & 1.00 & -2.40 \\
1.00 & 6.00 & 3.00 & 1.00 & 2.00 & 6.00 & -3.10 \\
2.00 & 3.00 & 4.00 & 1.00 & 2.00 & 3.00 & -4.20 \\
1.00 & 1.00 & 1.00 & 8.00 & 1.00 & 1.00 & -1.10 \\
1.00 & 2.00 & 2.00 & 1.00 & 10.00 & 2.00 & -2.10 \\
1.00 & 6.00 & 3.00 & 1.00 & 2.00 & 106.00 & -3.10 \\
-2.40 & -3.10 & -4.20 & -1.10 & -2.10 & -3.10 & 7.44
\end{array}\right]$$

after re-observing $f_1$:

$$\mu = \left[ \begin{array}{c}
2.95\\
2.43\\
3.18\\
12.03\\
15.07\\
4.97\\
-8.30\\
\end{array}\right],
\Sigma = \left[ \begin{array}{c c c c c c c}
-0.94 & -1.31 & -1.67 & -0.44 & -0.85 & 2.00 & 1.93 \\
-1.31& -3.66 & -2.82 & -0.83 & -1.62 & -4.08 & 4.19 \\
-1.67 & -2.82 & -3.10 & -0.84 & -1.62 & 1.73 & 3.83 \\
-0.44 & -0.83 & -0.84 & -0.23 & -0.45 & 0.15 & 1.08 \\
-0.85 & -1.62 & -1.62 & -0.45 & -0.87 & 0.16 & 2.09 \\
2.00 & -4.08 & 1.73 & 0.15 & 0.16 & -29.89 & 1.54 \\
1.93 & 4.19 & 3.83 & 1.08 & 2.09 & 1.54 & -5.18\\
\end{array}\right]$$

\subparagraph{iv} Compute the mean and covariance of $f_1$:
$$\mu = \left[ \begin{array}{c}
x_0 + rcos(\theta_0 + \phi) \\
y_0 + rsin(\theta_0 + \phi) \\
\end{array}\right]$$
$$J =  \left[ \begin{array}{c c}
cos(\theta_0 + \phi) & -rsin(\theta_0 + \phi)\\
sin(\theta_0 + \phi) & rcos(\theta_0 + \phi)\\
\end{array}\right]$$

\subparagraph{v} Update the landmark mean and covariance given re-observation of $f_1$:
$$K = \Sigma_0J^T\left(J\Sigma_0J^T + \Sigma_w\right)^{-1}$$
$$\mu_1 = \mu_0 + Kr$$
$$\Sigma_1 = (I-KJ)\Sigma_0$$
where $\mu_0$ and $\Sigma_0$ are the mean and covariance before this observation and $\mu_1$ and $\Sigma_1$ are the new mean and covariance.

\subparagraph{vi}
After observing $f_1$: \\
$$\mu_0 = \left[ \begin{array}{c}
3 \\
-8
\end{array}\right],
\Sigma_0 = \left[ \begin{array}{c c}
100 & 0 \\
0 & 3
\end{array}\right]$$

after re-observing $f_1$:
$$\mu_1 = \left[ \begin{array}{c}
4.57 \\
-8.5
\end{array}\right],
\Sigma_1 = \left[ \begin{array}{c c c c c c c}
50 & 0 \\
0 & 1.5
\end{array}\right]$$

% Task 1 part B-D
\paragraph{B}
FastSLAM 1.0 samples new robot positions from a distrubtion taking only
odometry measurements into account. However, it is often the case that
a robot's sensors are more accurate than its odometry and control. FastSLAM 2.0
leverages these accurate observation measurements to its advantage, sampling
new poses based on odometry \emph{and} observation measurements. This prevents
FastSLAM 2.0 from sampling as many low likelihood particles, making it more
efficient.

\paragraph{C}
Our particle resampling method resamples particles whenever a new observation
is made, since this is when the weights change. We iterate through all
particles to sum their weights, and then we draw random values between 0 and
the summed weight to select new particles. To determine which particle to pick,
we iterate through the particles, summing the weights again as we go, and when
the sum is greater than our random value, we return a copy of that particle.

To test for sampling bias, we can easily just sample an excessive amount of
particles and compare the resulting distribution of particles to our
expected distribution. If we sample many, many particles, large differences
in the resulting set of particles compared to our expected set will become
increasingly improbable and thus detectable with a certain confidence.

\paragraph{D}
Since every particle is an independent entity, particles that make bad
associations will be killed off during resampling. Therefore, our
expectation is that a particle-based method like FastSLAM can get away
with a fairly poor data association technique like nearest-neighbor
because only good associations will survive. A method like least-squares
SLAM commits to the poor associations it makes and may never recover, so
data association techniques must be ``smarter'' than nearest-neighbor
matching.

\paragraph{E}
Attached in email.

%XXX Make visualization key

\paragraph{F}
FastSLAM is easy to implement and does not require very ``intelligent'' data
association techniques to work well. In fact, a big advantage of FastSLAM is
that it is more robust to poor data associations than alternative SLAM
algorithms. Least-squares may run faster than FastSLAM, given the potentially
large particle requirements of FastSLAM, and it is more consistent in that
there is no random element to the problem. Least-squares also maintains
complete information forever, unlike FastSLAM, so it will not throw away
potentially useful information as FastSLAM does in cases involving particle
depletion. In other words, least-squares is good at loop closures.

FastSLAM is a safe choice in environments where data association is difficult.
If one can afford enough particles, it is probable that a particle that makes
the correct associations and can survive to give a good map. Least-squares SLAM
would likely not recover from a bad association without interference. However,
in cases where we are confident in our data association, methods like
least-squares SLAM converge faster due to the preservation of loops and, given
the optimizations employed in the past few years, are otherwise comparable to
FastSLAM in terms of speed of computation.

SLAM research has likely left particle filters behind because, given the option,
it is preferable to represent the entire problem, that is, to not throw out
constraint information, rather than rely on our best solutions to survive
filtering. %XXX More could be done here

%% Task 2
\section{Line Estimation from Laser Data}

\paragraph{A}Deriving a closed-form expression that computes the MSE:
Given 
$$M_x = \displaystyle\sum_i x_i \qquad M_{xx} = \displaystyle\sum_i x_i^2 \qquad q_x = \frac{M_x}{N}$$
$$M_y = \displaystyle\sum_i y_i \qquad M_{yy} = \displaystyle\sum_i y_i^2 \qquad q_y = \frac{M_y}{N}$$
 $$\hat{n} = \left[ \begin{array}{c}
-sin\theta \\
cos\theta
\end{array}\right]$$
 
\begin{align*} MSE &=  \displaystyle\sum_i \left[\left(x_i - q_x)\hat{n_x} + (y_i - q_y)\hat{n_y}\right)\right]^2\\
&=\displaystyle\sum_i \left[-\left(x_i - \frac{M_x}{N}\right)sin\theta + \left(y_i - \frac{M_y}{N}\right)cos\theta\right]^2 \\
&=\displaystyle\sum_i \left(\left(y_i - \frac{M_y}{N}\right)cos^2\theta \right) 
-2\displaystyle\sum_i \left(\left(x_i-\frac{M_x}{N}\right)\left(y_i - \frac{M_y}{N}\right)cos\theta sin\theta\right) \\
 &\qquad+ \displaystyle\sum_i \left(\left(x_i - \frac{M_x}{N}\right)cos^2\right) \\
&= cos^2\theta(M_{yy}-\frac{M_y^2}{N}) - 2cos\theta sin\theta(M_{xy}-\frac{M_xM_y}{N}) + sin^2\theta(M_{xx}-\frac{M_x^2}{N})
\end{align*}

\paragraph{B}

\paragraph{C}
% XXX Keep trying, folks

%% Task 3
\section{RANSAC Rigid-Body Transformation}

\paragraph{A}

\paragraph{B}

\paragraph{C}

%% Task 4
\section{Advanced Data Association }

\end{document}
